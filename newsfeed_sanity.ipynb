{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "from gensim.parsing.preprocessing import preprocess_string, remove_stopwords, stem, strip_non_alphanum, strip_numeric, strip_tags, strip_punctuation, strip_short\n",
    "CUSTOM_FILTERS = [lambda x: x.lower(), remove_stopwords, \n",
    "                  stem, strip_non_alphanum, strip_numeric, \n",
    "                  strip_tags, strip_punctuation, strip_short]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ~/Library/Application\\ Support/Google/Chrome/Default/History ./ # Google Chrome history database location on Mac OS X\n",
    "## please change the location according to your OS (Linux/Winwin/MAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlite query to extract the browsing data\n",
    "!mv ./chrome_history.txt ./chrome_history_temp.txt\n",
    "!sqlite3 History \"select datetime(last_visit_time/1000000-11644473600,'unixepoch','localtime'),url from  urls where last_visit_time > 0 order by last_visit_time desc\" >> ./chrome_history.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-format the data\n",
    "\n",
    "# Open file\n",
    "with open('chrome_history.txt') as f:\n",
    "    content = f.readlines()\n",
    "# Strip whitespace then split on first occurrence of pipe character\n",
    "raw_data = [line.split('|', 1) for line in [x.strip() for x in content]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(raw_data, columns=['datetime', 'url'])\n",
    "data['pages_desc'] = data.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.datetime = pd.to_datetime(data.datetime.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse the URLs\n",
    "from urllib.parse import urlparse\n",
    "netloc_parser = lambda u: urlparse(u).netloc\n",
    "data.url = data.url.apply(netloc_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>url</th>\n",
       "      <th>pages_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2018-05-21 18:10:30</td>\n",
       "      <td>m.phys.org</td>\n",
       "      <td>https://m.phys.org/news/2018-05-weve-nucleolus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2018-05-21 18:10:30</td>\n",
       "      <td>m.phys.org</td>\n",
       "      <td>https://m.phys.org/news/2018-05-magnonic-inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2018-05-21 18:10:30</td>\n",
       "      <td>m.phys.org</td>\n",
       "      <td>https://m.phys.org/news/2018-05-chemistry-smar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               datetime         url  \\\n",
       "145 2018-05-21 18:10:30  m.phys.org   \n",
       "146 2018-05-21 18:10:30  m.phys.org   \n",
       "147 2018-05-21 18:10:30  m.phys.org   \n",
       "\n",
       "                                            pages_desc  \n",
       "145  https://m.phys.org/news/2018-05-weve-nucleolus...  \n",
       "146  https://m.phys.org/news/2018-05-magnonic-inter...  \n",
       "147  https://m.phys.org/news/2018-05-chemistry-smar...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the website of interests and the category \n",
    "# (works for m.phys.org, change according to the targeted website, \n",
    "# e.g. have a look to the page source from Chrome)\n",
    "# You should have some historic on 'm.phys.org' for it to work\n",
    "key_website = 'm.phys.org'\n",
    "bool_website = data.pages_desc.str.contains(key_website)\n",
    "key_pages = '/news/'\n",
    "bool_pages = data.pages_desc.str.contains(key_pages)\n",
    "data[bool_website&bool_pages].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_list = data[bool_website&bool_pages].pages_desc.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://m.phys.org/news/2018-05-weve-nucleolus-left-school.html',\n",
       " 'https://m.phys.org/news/2018-05-magnonic-interferometer-paves-energy-efficient-devices.html',\n",
       " 'https://m.phys.org/news/2018-05-chemistry-smart-drugs-smarter.html']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the topics only (title here)\n",
    "topics_list = list()\n",
    "for weblink in pages_list:\n",
    "    topics_list.append(str(BeautifulSoup(urllib.request.urlopen(weblink),\"html5lib\").title.string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What we've learned about the nucleolus since you left school\",\n",
       " 'Magnonic interferometer paves way toward energy-efficient information processing devices',\n",
       " 'Researchers develop new chemistry to make smart drugs smarter']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "print(len(topics_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec model, here GoogleNews is used\n",
    "# warning: *.bin can be heavy for your RAM (tried on 8GB RAM without any problem or drag)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the topics representation from their components (words)\n",
    "\n",
    "index2word_set = set(model.index2word)\n",
    "\n",
    "# sentence/topic representation through averaging of the words representation\n",
    "def avg_feature_vector(words, model=model, num_features=300, index2word_set=index2word_set):\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model.get_vector(word))\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['learn', 'nucleolu', 'left', 'school'],\n",
       " ['magnon',\n",
       "  'interferomet',\n",
       "  'pave',\n",
       "  'wai',\n",
       "  'energy',\n",
       "  'effici',\n",
       "  'inform',\n",
       "  'process',\n",
       "  'devic'],\n",
       " ['research', 'develop', 'new', 'chemistri', 'smart', 'drug', 'smarter']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-process the topics list for avoiding unuseful words/particles\n",
    "preprocess_topics_list = list()\n",
    "for itopics in topics_list:\n",
    "    preprocess_topics_list.append(preprocess_string(itopics, CUSTOM_FILTERS))\n",
    "preprocess_topics_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_list_prev = list()\n",
    "with open('./topics_list.txt','r') as fp:\n",
    "    for itopics in fp:\n",
    "        topics_list_prev.append(itopics.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics_list_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the new topics with previous for uniqueness check and saving\n",
    "topics_list = [list(i) for i in set(tuple(i) for i in (preprocess_topics_list+topics_list_prev))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['amateur', 'mathematician', 'partial', 'solv', 'year', 'old', 'problem'],\n",
       " ['evolut', 'language', 'there', 'app'],\n",
       " ['artifici', 'intellig', 'acceler', 'discoveri', 'metal', 'glass']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save/re-write in memory a topics list\n",
    "!rm -f ./topics_list.txt\n",
    "with open('./topics_list.txt','w') as fp: \n",
    "    for itopics in topics_list:\n",
    "        fp.write(' '.join(itopics)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract topics title, URLs\n",
    "topics_list_new = list()\n",
    "page_titles_list = list()\n",
    "page_urls_list = list()\n",
    "max_page_number = 1\n",
    "for page_number in range(1,max_page_number+1):\n",
    "    page_titles = BeautifulSoup(urllib.request.urlopen('http://m.phys.org/page{}.html'.format(page_number)),\n",
    "                                \"html5lib\").find_all('h3', {\"class\": \"ui-li-heading\"})\n",
    "    page_urls = BeautifulSoup(urllib.request.urlopen('http://m.phys.org/page{}.html'.format(page_number)),\n",
    "                                \"html5lib\").find_all('a', {\"class\": \"ui-link-inherit list-img\"}, href=True)\n",
    "    for page_title in page_titles:\n",
    "        page_titles_list.append(str(page_title.text))\n",
    "        topics_list_new.append(preprocess_string(str(page_title.text), CUSTOM_FILTERS))\n",
    "    for c, page_url in enumerate(page_urls):\n",
    "        page_urls_list.append('<a href=\"{}\">{}</a>'.format(page_url['href'], page_titles[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the distances between past and new topics in the space of representation\n",
    "\n",
    "score_list = list()\n",
    "\n",
    "s1_afv = list()\n",
    "for topic in topics_list:\n",
    "    s1_afv.append(avg_feature_vector(topic))\n",
    "    \n",
    "s2_afv = list()\n",
    "for topic in topics_list_new:    \n",
    "    s2_afv.append(avg_feature_vector(topic))\n",
    "    \n",
    "for vec2 in s2_afv:\n",
    "    score_list_tmp = list()\n",
    "    for vec1 in s1_afv:\n",
    "        if np.sum(np.power(vec2,2)) != 0. and np.sum(np.power(vec1,2)) != 0.:\n",
    "            sim = 1-spatial.distance.cosine(np.array(vec1), np.array(vec2)) # distance/similarity between articles' topics\n",
    "        else:\n",
    "            sim = 0.\n",
    "        score_list_tmp.append(sim)\n",
    "    score_list.append(np.max(score_list_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a dataframe\n",
    "importance_article = pd.DataFrame(\n",
    "                                 {\n",
    "                                     'page_titles': page_titles_list,\n",
    "                                     'score': score_list, \n",
    "                                     'page_urls': page_urls_list\n",
    "                                 }\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_titles</th>\n",
       "      <th>page_urls</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nanoparticles could offer a new way to help eradicate polio worldwide</td>\n",
       "      <td>&lt;a href=\"https://m.phys.org/news/2018-05-nanoparticles-eradicate-polio-worldwide.html\"&gt;&lt;h3 class=\"ui-li-heading\"&gt;Nanoparticles could offer a new way to help eradicate polio worldwide&lt;/h3&gt;&lt;/a&gt;</td>\n",
       "      <td>0.620211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Major fossil study sheds new light on emergence of early animal life 540 million years ago</td>\n",
       "      <td>&lt;a href=\"https://m.phys.org/news/2018-05-major-fossil-emergence-early-animal.html\"&gt;&lt;h3 class=\"ui-li-heading\"&gt;Major fossil study sheds new light on emergence of early animal life 540 million years ago&lt;/h3&gt;&lt;/a&gt;</td>\n",
       "      <td>0.556796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A better way to control crystal vibrations</td>\n",
       "      <td>&lt;a href=\"https://m.phys.org/news/2018-05-crystal-vibrations.html\"&gt;&lt;h3 class=\"ui-li-heading\"&gt;A better way to control crystal vibrations&lt;/h3&gt;&lt;/a&gt;</td>\n",
       "      <td>0.552698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Chemists synthesize millions of proteins not found in nature</td>\n",
       "      <td>&lt;a href=\"https://m.phys.org/news/2018-05-chemists-millions-proteins-nature.html\"&gt;&lt;h3 class=\"ui-li-heading\"&gt;Chemists synthesize millions of proteins not found in nature&lt;/h3&gt;&lt;/a&gt;</td>\n",
       "      <td>0.520514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Humans account for little next to plants, worms, bugs</td>\n",
       "      <td>&lt;a href=\"https://m.phys.org/news/2018-05-humans-account-worms-bugs.html\"&gt;&lt;h3 class=\"ui-li-heading\"&gt;Humans account for little next to plants, worms, bugs&lt;/h3&gt;&lt;/a&gt;</td>\n",
       "      <td>0.508767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  page_titles  \\\n",
       "4  Nanoparticles could offer a new way to help eradicate polio worldwide                        \n",
       "5  Major fossil study sheds new light on emergence of early animal life 540 million years ago   \n",
       "2  A better way to control crystal vibrations                                                   \n",
       "6  Chemists synthesize millions of proteins not found in nature                                 \n",
       "0  Humans account for little next to plants, worms, bugs                                        \n",
       "\n",
       "                                                                                                                                                                                                          page_urls  \\\n",
       "4  <a href=\"https://m.phys.org/news/2018-05-nanoparticles-eradicate-polio-worldwide.html\"><h3 class=\"ui-li-heading\">Nanoparticles could offer a new way to help eradicate polio worldwide</h3></a>                    \n",
       "5  <a href=\"https://m.phys.org/news/2018-05-major-fossil-emergence-early-animal.html\"><h3 class=\"ui-li-heading\">Major fossil study sheds new light on emergence of early animal life 540 million years ago</h3></a>   \n",
       "2  <a href=\"https://m.phys.org/news/2018-05-crystal-vibrations.html\"><h3 class=\"ui-li-heading\">A better way to control crystal vibrations</h3></a>                                                                    \n",
       "6  <a href=\"https://m.phys.org/news/2018-05-chemists-millions-proteins-nature.html\"><h3 class=\"ui-li-heading\">Chemists synthesize millions of proteins not found in nature</h3></a>                                   \n",
       "0  <a href=\"https://m.phys.org/news/2018-05-humans-account-worms-bugs.html\"><h3 class=\"ui-li-heading\">Humans account for little next to plants, worms, bugs</h3></a>                                                  \n",
       "\n",
       "      score  \n",
       "4  0.620211  \n",
       "5  0.556796  \n",
       "2  0.552698  \n",
       "6  0.520514  \n",
       "0  0.508767  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_article = importance_article.sort_values(by=['score'], ascending=False) # rank by the score\n",
    "importance_article = importance_article[importance_article.score<1.] # avoid already read articles\n",
    "importance_article.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the dataframe as an html page with clickable hyperlinks\n",
    "old_width = pd.get_option('display.max_colwidth')\n",
    "pd.set_option('display.max_colwidth', -1) # avoid the truncation of the urls\n",
    "importance_article.drop(['page_titles'],axis=1).set_index('score').to_html('new_topics_list.html', escape=False)\n",
    "webbrowser.open_new('file:///Users/<user_name>/<working_directory>/new_topics_list.html') # change for your OS\n",
    "pd.set_option('display.max_colwidth', old_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the result is not beautiful but useful. Please, feel free to fork it, change it to your tastes\n",
    "## I know there are redundancies in the code, but I'm sure you'll simply improve it ;) \n",
    "## Have fun and a sane newsfeed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
